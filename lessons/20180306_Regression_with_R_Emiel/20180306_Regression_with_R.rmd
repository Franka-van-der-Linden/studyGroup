---
title: "Regression analysis in R"
author: "Emiel van Loon"
date: "March 6th, 2018"
output:
html_document:
keep_md: true
---

# Linear Regression 

> Linear regression is used to predict the value of an outcome variable $Y$ based on one or more input predictor variables $X$. The aim is to establish a linear relationship (a mathematical formula) between the predictor variable(s) and the response variable, so that, we can use this formula to estimate the value of the response $Y$, when only the predictors ($Xs$) values are known.

##Introduction

The aim of linear regression is to model a continuous variable $Y$ as a mathematical function of one or more $X$ variable(s), so that we can use this regression model to predict the $Y$ when only the $X$ is known. This mathematical equation can be generalized as follows:

$$Y = \beta_{1} + \beta_{2} X + \epsilon$$

where, $\beta_{1}$ is the intercept and $\beta_{2}$ is the slope. Collectively, they are called _regression coefficients_. $\epsilon$ is the error term, the part of $Y$ the regression model is unable to explain.

<!-- ![regression_model]("./screenshots/linear-regression-small.png", dpi = 220) -->


## Example Problem

In this lesson, we will use a data set where the dry matter
yield of willow (Salix spp) was measuerd along with several size-measures

Accurate non-destructive measurement of dry matter is time consuming. Still it is 
desirable to have such estimates, e.g. to monitor production.
Therefore the idea arose to search for easy to measure surrogate variables.
And a few of such variables are included in the willows data set.

The data set is loaded by the following commands.

```{r echo = TRUE, eval = TRUE}
require(RCurl)
W <- read.csv(text=getURL("https://raw.githubusercontent.com/ScienceParkStudyGroup/studyGroup/gh-pages/lessons/20180306_Regression_with_R_Emiel/willow.csv"), header=T)
```


The dataframe `W`, which has the following variables:

- ID - a sample number, assigned in the field 	
- DryMatter - the dry matter of the harvestable stems from the tree in kg
-	SumLength - the sum of lengths of all stems in cm
-	SumDiam - the sum of diameters of all stems in cm
-	MaxLength - the length of the longest stem in cm
-	LengthTop5 - the average length of the five longest stems in cm


Lets print out the first six observations from W.

```{r echo = TRUE, eval = TRUE}
head(W)
```

Before we begin building the regression model, it is a good practice to analyze and understand the variables. The graphical analysis and correlation study below will help with this.

##Graphical Analysis

The aim of this exercise is to build a simple regression model that we can use to predict the dry matter (DryMatter) by establishing a statistically significant linear relationship with one of the size-variables (SumLength, SumDiam, Maxlength and LengthTop5). But before jumping in to the syntax, lets try to understand these variables graphically.

Typically, for simple regression, you would like to make a scatter plot of the relation first.
In one view it shows whether the overall relation between the predictor and response is linear, and also if there are any strange points in the data set, which deviate e.g. a lot from the main trend.

Scatter plots can help visualize any linear relationships between the dependent (response) variable and independent (predictor) variables. Ideally, if you are having multiple predictor variables, a scatter plot is drawn for each one of them against the response, along with a trendline.

The plotting function `scatter.smooth` allows to draw a scatterplot with a non-parametric trendline (a so-called loess smoother) drawn into it. This plot gives a quick impression about the linearity of the relation that is being considered. 

```{r echo=TRUE, eval=FALSE}
scatter.smooth(x=W$SumLength, y=W$DryMatter, main="DryMatter ~ SumLength",
               xlab = 'sum of length all stems (cm)',
               ylab = 'dry matter of all stems (kg)')  
```

In this case, the scatter plot along with the smoothing line shows a bend and suggests that a non-linear relationship may be more appropriate.

**Question:** How can you apply the plot command to the other potential predictors as well to check which of these might me most promising to predict the total Dry Matter harvest.  


## Correlation

Correlation (or in fact the "Pearson product-moment correlation") is a statistical measure that describes the level of linear dependence between two variables - just like what we have here in DryMatter and SumLength. Correlation can take values between -1 to +1.
If we observe for every instance where DryMatter increases, the SumLength also increases along with it, then there is a high positive correlation between them and therefore the correlation between them will be closer to 1. The opposite is true for an inverse relationship, in which case, the correlation between the variables will be close to 1. 

A value closer to 0 suggests a lot of scatter around the linear relationship between the variables or a non-linear relationship! The scatter plot already gave a clear indication about this. 

Important points are worth noticing with regard to the Pearson Correlation:

- Eventhough it gives a 'scaled statistic' (scaled, becauase it is always between -1 and 1),
there is no general yardstick of what is a high and a low correlation coefficient; what is considered large or small depends a lot on the type of data.
- There is an inimite relation between the correlation coefficient and linear regression (as will be shown later), however they are not measuring the same. When correlating two variables their order does not matter (we don't call one the predictor an the other the response). When fitting a regression model, order does matter!

```{r, eval=TRUE}
cor(W$SumLength, W$DryMatter)
```

**Question:** Try to guess the correlation coefficient for one or more of the other scatter plots you have created and subsequently calculate it to check your guess.


## Build Linear Model

Now that we have seen the linear relationship pictorially in the scatter plot and also have an idea bout the strength of the linear relationship by computing the correlation, lets see the syntax for building the linear model. The function used for building linear models is `lm()`. The `lm()` function takes in two main arguments, namely: 1. Formula  2. Data. The data is typically a data.frame and the formula is a object of class `formula`. But the most common convention is to write out the formula directly in place of the argument as shown below.

```{r, eval=TRUE}
linMod <- lm(DryMatter ~ SumLength, data=W)
linMod
```

So now we also have established the relationship between the predictor and response in the form of a mathematical formula for DryMatter as a function of SumLength
In the model-output you can notice the 'Coefficients' part having two components: *Intercept*: 7.447, *SumLength*: 0.001
These are also called the beta coefficients. In other words,
**$$DryMatter = Intercept + \beta*SumLength$$**
**$$DryMatter = 7.447 + 0.001*SumLength$$**

The slope-coefficient is quite small. By changing the units of the variables, the values of this coeficient would change. But such a conversion (a linear transformation of a variable) will not change the shape of the relationship.

**Question:** Change either the units of DryMatter or SumLength such that the value of the slope coeffcient becomes bigger.


## Linear Regression Diagnostics

Now the linear model is built and we have a formula that we can use to predict the dist value if a corresponding speed is known. Is this enough to actually use this model? NO!
Before using a regression model, you have to ensure that:

- it is statistically significant.
- the assumptions underlying the model are met

How do you ensure this? Lets begin by printing the summary statistics for linMod.
```{r, eval=TRUE}
summary(linMod)
```

#### The model p-Value: overall statistical significance

The summary statistics above tell us a number of things. Let's start at the very bottom, where the model p-value is given. (Notice that it is quite small in this case.)

A p-value specifies the probability that a result as extreme as in this experiment (or more extreme) is found, if the real system is behaving as under the null-hypothesis.

Hence a p-value implies that some null-hypothesis has been specified. For a linear regression model this null-hypothesis is that the linear model does explain the variation in the response variable. The alternative hypothesis is that the linear model does explain some of the variation in the resonse variable. So, a very small p-value implies that the situation under the null-hypothesis is unlikely.
Quite commonly, a significance value of 0.05 is chosen (but this shouldn't be an automatism as a matter of fact). And if the p-value is smaller than this threshold the null-hypothesis of no-effect is rejected.


#### t-value

In the coefficient-table, the *Estimate* is shown, followed by the standard error
of the estimate (*Std. Error*) and t-values (*t value*) 
A larger *t-value* indicates that it is less likely that the coefficient is not equal to zero purely by chance.
*Pr(>|t|)* or _p-value_ is the probability that you get a t-value as high or higher than the observed value when the Null Hypothesis (the $\beta$ coefficient is equal to zero or that there is no relationship) is true. So if the *Pr(>|t|)* is low, the coefficients are significant (significantly different from zero). If the *Pr(>|t|)* is high, the coefficients are not significant.

What does this means to us? For the model-intercept, the question whether the estimated value is different from zero is usually not interesting (it is not interesting in this case). In contrast, the question whether a slope-coefficient is different from zero *is* interesting. It translates directly into the idea that there is a relationship between the independent variable in question and the dependent variable.

It is absolutely important for the model to be statistically significant before we can go ahead with thecking assumptions and use it to predict (or estimate) the dependent variable, otherwise, the confidence in predicted values from that model reduces and may be construed as an event of chance.

## How to calculate the t Statistic and p-Values?

When the model co-efficients and standard error are known, the formulas for calculating t Statistic and p-Value is as follows:
$$t~Statistic = {(\beta-coefficient value) \over Std.Error}$$
```{r, eval=TRUE}
modelSummary <- summary(linMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
beta.estimate <- modelCoeffs["SumLength", "Estimate"]  # get beta estimate for speed
std.error <- modelCoeffs["SumLength", "Std. Error"]  # get std.error for speed
t_value <- beta.estimate/std.error  # calc t statistic
p_value <- 2*pt(-abs(t_value), df=nrow(W)-2)  # calc p Value
f_statistic <- linMod$fstatistic[1]  # fstatistic
f <- summary(linMod)$fstatistic     # parameters for model p-value calc
model_p <- pf(f[1], f[2], f[3], lower=FALSE)
```
```{r, echo=TRUE}
cat("\nslope t-value: ", t_value)
cat("\nslope p-value: ", p_value)
cat("\nModel F Statistic: ", f[1], f[2], f[3])
cat("\nModel p-value: ", model_p)
```



## R-Squared and Adj R-Squared

What R-Squared tells us is the proportion of variation in the dependent (response) variable that has been explained by this model. It is defined in the following way.

$$ R^{2} = 1 - \frac{SSE}{SST}$$

where, $SSE$ is the _sum of squared errors_ given by $SSE =  \sum_{i}^{n} \left( y_{i} - \hat{y_{i}} \right) ^{2}$ and $SST =  \sum_{i}^{n} \left( y_{i} - \bar{y_{i}} \right) ^{2}$ is the _sum of squared total_. Here, $\hat{y_{i}}$ is the fitted value for observation $i$ and $\bar{y}$ is the mean of $Y$.

Interesingly, the R-squared can also be calculated via another route: by squaring the pearson correlation coefficient.

**Question:** Calculate the R-squared via the correlation coefficient and check that it gives the same value as the output of linMod.


__Now thats about R-Squared. What about adjusted R-Squared?__ 

Eventhough we will only consider a model with one predictor varialbe in this lesson, if you would you add more predictor-variables to your model, the R-Squared value of the new bigger model will always be greater than that of the smaller subset. This is because additional variables can only add (if not significantly) to the variation that was already explained.
However sometimes this additional explanatory power is negligible. It is here, the adjusted R-Squared value comes to help. Adj R-Squared penalizes total value for the number of parameters (one for each predictors) in your model. 
Therefore when comparing models with a different number of predictors (and they should be nested in fact), the adj-R-squared value should be used in stead of R-squared.

$$ R^{2}_{adj} = 1 - \frac{MSE}{MST}$$

where, $MSE$ is the _mean squared error_ given by $MSE =  \frac{SSE}{\left( n-q \right)}$ and $MST = \frac{SST}{\left( n-1 \right)}$ is the _mean squared total_, where $n$ is the number of observations and $q$ is the number of coefficients in the model.

By moving around the numerators and denominators, the relationship between $R^{2}$ and $R^{2}_{adj}$ becomes:

$$R^{2}_{adj} =  1 - \left( \frac{\left( 1 - R^{2}\right) \left(n-1\right)}{n-q}\right)$$

## Standard Error and F-Statistic

Both standard errors and F-statistic are measures of goodness of fit.

$$Std. Error = \sqrt{MSE} = \sqrt{\frac{SSE}{n-q}}$$

$$F-statistic = \frac{MSR}{MSE}$$

where, $n$ is the number of observations, $q$ is the number of coefficients and $MSR$ is the _mean square regression_, calculated as, 

$$MSR=\frac{\sum_{i}^{n}\left( \hat{y_{i} - \bar{y}}\right)}{q-1} = \frac{SST - SSE}{q - 1}$$

## AIC and BIC

The Akaike's information criterion - AIC (Akaike, 1974) and the Bayesian information criterion - BIC (Schwarz, 1978) are measures of the goodness of fit of an estimated statistical model and can also be used for model selection. Both criteria depend on the maximized value of the likelihood function L for the estimated model.

The AIC is defined as:

$$AIC = \left( -2 \right) \times ln \left( L \right) + \left(2 \times k \right)$$

where, k is the number of model parameters and the BIC is defined as:

$$BIC = \left( -2 \right) \times ln \left( L \right) + k \times ln \left( n \right)$$

where, n is the sample size.

For model comparison, the model with the lowest AIC and BIC score is preferred.
```{r, eval=FALSE}
AIC(linMod) 
BIC(linMod) 
```

**Question:** Let's compare linear models with different predictor variables (in total there are 4 in W). And see which model leads to the lowest AIC value.
Is that the same model that would be selected if we would use R-squared as a criterion?


## Assumptions

> Building a linear regression model is only half of the work. In order to actually be usable in practice, the model should conform to the assumptions of linear regression.

###Assumption 1

>#### _The regression model is linear in parameters_

An example of model equation that is _linear in parameters_
$$Y = a + \left( \beta1 * X1 \right) + \left( \beta2 * X2^2 \right)$$

Though, the `X2` is raised to power 2, the equation is still linear in beta parameters. So the assumption is satisfied in this case.

## Assumption 2

>#### _The mean of residuals is zero_

####How to check?
Check the mean of the residuals. If it zero (or very close), then this assumption is held true for that model. When fitting linear models this should always be the case (if not, you ddid not apply the lm() function in the standard way). 
But let's just check it.

```{r, eval=TRUE}
mean(linMod$residuals)
```
Since the mean of residuals is approximately zero, this assumption holds true for this model.

## Assumption 3
>#### _Homoscedasticity of residuals or equal variance_

#### How to check?

Once the regression model is built, 
save the current parameter settings for the figure window
`par_current <- par()`
Next set `par(mfrow=c(2, 2))`, then, plot the model using `plot(lm.mod)`. This produces four plots. The _top-left_ and _bottom-left_ plots shows how the _residuals_ vary as the _fitted_ values increase.
Finally, set the parameter settings back to original

```{r, eval=FALSE}
par_current <- par()
par(mfrow=c(2,2))        # set 2 rows and 2 column plot layout
plot(linMod)
par <- par_current
```

From the first plot (top-left), as the fitted values along x increase, the residuals go from negative to zero and then back to negative again. This pattern is indicated by the red line. The pattern in this plot should match a flat line - but this is clearly not the case here.
The plot on the bottom left specifically checks whether the error distribution is equally wide for different levels of the fitted values (it is relatively easy to see any deviations as the disturbance term in Y axis is standardized). 
In this case, there is no definite pattern noticed. 

## Assumption 4
>#### _No autocorrelation of residuals_
This is applicable especially for time series data. Autocorrelation is the correlation of a time Series with lags of itself. When the residuals are autocorrelated, it means that the current value is dependent of the previous (historic) values and that there is a definite unexplained pattern in the Y variable that shows up in the disturbances.

We'll check this issue in a next session.


# Conclusion

In this lesson, we have seen how to fit and interpret a linear model
We have focussed on interpreting the statistical output and looked at 
model assumptions.

# To lear more about (simple and more advanced) aspects of
# regression modeling, some good resources are:
- [Modern Dive](http://moderndive.com/index.html)
- [ggplot2 cookbook](http://www.cookbook-r.com/Graphs/)


