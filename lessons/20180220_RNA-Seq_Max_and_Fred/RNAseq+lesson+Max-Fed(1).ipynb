{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this RNA-seq lesson, we will give a basic rundown of procces of going form your raw data to differential gene expression. For this we will use a couple of different tools: \n",
    "<b>Anaconda</b> - This will allow us to get the other packages easy without hickups. Download\n",
    "packages can be found by going to \"environments\" search environments \"bioconda\" then search package your package of choice.\n",
    "<b>FASTQC</b>  - to check the quality of the reads. We will not go in depth how to work with this package here, as a previous lesson already covers the subject:\n",
   " <b>FASTX-toolkit</b> - A toolkit to filter your .fasta files in multiple ways. An alternative is trimmomatic which has many of the same features.\n",
    "<br><b>Kallisto</b> - This is the alligner we will be using during this example.\n",</br>
    "<b>Sleuth</b> - the accompagnying software for Kallisto, this can be run from R(studio) or the shell, we will use Rstudio in the example.\n",
    "\n",
    "For each step we have providede example data, that can be downloaded from here. If you save all the data to a folder, and set this as your \"working directory\" all command should be able to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastQC\n",
    "We will not delve into the possibilities of FastQC here, for more information you can check out <a href=\"http://www.datacarpentry.org/wrangling-genomics/00-quality-control/\">This</a> lesson.\n",
    "In anaconda you can load the FASTQC package or download it <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\">here</a>\n",
    "The raw data files have some reads that have a low quality score, and some reads that are to small to allign properly to the transcripts, so the next step will be to remove these reads.\n",
    "pic\n",
    "pic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do 2 things with our sample, we want to remove all reads that are smaller than 25 nucleotides and have a quality score of 17 or less. We choose 17 in this case since we are working with data sequenced using ion-torrent sequencing. If your own data set has used some other sequencing platform you might want to use a different cut off value (illumina is around..)\n",
    "We will use the fastx-toolkit to do this. Make sure you have it installed first. And set your working directory to donwload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd user/download/RNAseqe\n",
    "# or to wherever you have put the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get rid of reads smaller than 25 nuclotides with the follwing command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fastx_clipper -l 25 -i ... -o ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same manner we can filter out the reads with low quality scores. In this command the q stands for the quality that we want and the p stands for the percentage of the reads that has to be this quality to be cut out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fastq_quality_filter -q 17 -p 75 -i ... -o ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know both commands we can put them together and loop them so we do the filtering for all .fasta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in fastq-files/S*.fastq; \n",
    "    do fastx_clipper -l 25 -i $filename | fastq_quality_filter -q 17 -p 75 -o filtered/$filename-filtered.fastq; \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kallisto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have filtered the data we can use it to allign to a reference genome. For this we will use kallisto. It is a fast and precise allginer.\n",
    "\n",
    "Kallisto can be run with just a single command. It requires: \n",
    "-i a reference genome\n",
    "-o a place to put the output <b>folder</b>\n",
    "-b the number of bootstraps kallist will perform (the number of times the allignemnt will be refined)\n",
    "-- single (or paired if you have paired reads)\n",
    "-l the avarge length of your reads\n",
    "-s the standard deviation of your reads\n",
    "and lastly the file you want to allign.\n",
    "\n",
    "The -l and -s can be extracted from you data set using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in filtered/*.fastq; do cat $filename | awk '{if(NR%4==2) print length($1)}' > $filename-readslength.txt; done\n",
    "#this will count the number of characters in every 4th row (the row with the basepairs) and print them in an output file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#these files can now be loaded into R. Where you can extract the mean and SD. Be sure to first set your working directory. This proccs, I am sure can be looped but I can't for the life of me figure out how. so we will just use the power op \"copy\" and \"paste\" \n",
    "\n",
    "in R.\n",
    "x1 <- read.csv(\"S01.fastq-filtered.fastq-readslength.txt\", header = FALSE)\n",
    "x2 <- read.csv(\"S02.fastq-filtered.fastq-readslength.txt\", header = FALSE)\n",
    "x3 <- read.csv(\"S03.fastq-filtered.fastq-readslength.txt\", header = FALSE)\n",
    "x4 <- read.csv(\"S04.fastq-filtered.fastq-readslength.txt\", header = FALSE)\n",
    "x5 <- read.csv(\"S05.fastq-filtered.fastq-readslength.txt\", header = FALSE)\n",
    "x6 <- read.csv(\"S06.fastq-filtered.fastq-readslength.txt\", header = FALSE)\n",
    "mean(x1$V1); sd(x1$V1)\n",
    "mean(x2$V1); sd(x2$V1)\n",
    "mean(x3$V1); sd(x3$V1)\n",
    "mean(x4$V1); sd(x4$V1)\n",
    "mean(x5$V1); sd(x5$V1)\n",
    "mean(x6$V1); sd(x6$V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if all your read lengths and sd are the same (this will happen in the case of illumina) you can loop it.\n",
    "for filename in filtered/S*.fastq; \n",
    "do kallisto quant -i practice/TAIR10_cdna_20101214_updated_ERCC92.idx -o alligned/$filename -b 500 --single -l 90 -s 30 filtered/$filename; \n",
    "done\n",
    "\n",
    "#in our case the read length and SD is different in every sample, so we will copy paste the command\n",
    "#running this might take some time, as the library made by kallisto is that of the whole arabidopsis genome everytime. Kallisto will tell you whenever it switches to a different bootstrap, so you can see how long it takes to finishes 1 alligenment.\n",
    "kallisto quant -i TAIR10_cdna_20101214_updated_ERCC92.idx -o alligned/S01 -b 5 --single -l 87.47 -s 36.41 filtered/S01.fastq\n",
    "kallisto quant -i TAIR10_cdna_20101214_updated_ERCC92.idx -o alligned/S02 -b 5 --single -l 87.47 -s 36.41 filtered/S02.fastq\n",
    "kallisto quant -i TAIR10_cdna_20101214_updated_ERCC92.idx -o alligned/S03 -b 5 --single -l 87.47 -s 36.41 filtered/S03.fastq\n",
    "kallisto quant -i TAIR10_cdna_20101214_updated_ERCC92.idx -o alligned/S04 -b 5 --single -l 87.47 -s 36.41 filtered/S04.fastq\n",
    "kallisto quant -i TAIR10_cdna_20101214_updated_ERCC92.idx -o alligned/S05 -b 5 --single -l 87.47 -s 36.41 filtered/S05.fastq\n",
    "kallisto quant -i TAIR10_cdna_20101214_updated_ERCC92.idx -o alligned/S06 -b 5 --single -l 87.47 -s 36.41 filtered/S06.fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our allignements we can find how many reads have alligned to every transcript in the \"abundance.tsv\" files. You can check out the differential expression in your favorite tool, including excel. But we will continue in SLeuth as it is uniquely capable of using the bootstrapping information (abundance.h5) to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sleuth is a package that is run in R. Sleuth can compare your sample in 2 ways, using a likelyhood ratio test(lrt), or a wald test(wt). The Wald test will give you a fold change between genes, so that is the test we will use in this case.\n",
    "We will now go step by step through the procces of doing a wt test. You can copy each piece into R after each other so you can see what everyhting does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load sleuth\n",
    "suppressMessages({\n",
    "  library(\"sleuth\")\n",
    "})\n",
    "\n",
    "#set work directory yourself first\n",
    "setwd(\"~/Documents/RNA-seq\")\n",
    "\n",
    "#make folders to save to\n",
    "dir.create(file.path(\"sleuth_output \",\"WT\"))\n",
    "dir.create(file.path(\"sleuth_output \",\"WT-significant\"))\n",
    "\n",
    "#First Sleuth needs to know where the files are located.\n",
    "sample_id <- dir(file.path(\"alligned\"))\n",
    "\n",
    "#Next we make a file in which the filepaths are located.\n",
    "kal_dirs <- file.path(\"alligned\", sample_id)\n",
    "\n",
    "#We give sleuth the experimental set up\n",
    "s2c <- read.table(file.path(“experimental_setup.txt”), header = TRUE, stringsAsFactors=FALSE)\n",
    "#We tell it over which variable to find the difference. In this case it will “condition”\n",
    "s2c <- dplyr::select(s2c, sample = sample, “condition”)\n",
    "#add filepath for files in pathname so R knows which file corresponds to which metadata entry\n",
    "s2c <- dplyr::mutate(s2c, path = kal_dirs)\n",
    "\n",
    "#You can visualize the addition by looking at the table\n",
    "s2c\n",
    "\n",
    "\n",
    "#Before we can do the WT test, we need change the s2c file  so that is can easily be read by sleuth.\n",
    "condfactor <- s2c[,”condition”]\n",
    "variablebaseline <- paste (condition,baseline, sep = \"\", collapse = NULL)\n",
    "#makes a table for WT test\n",
    "cond <- factor(condfactor)\n",
    "cond <- relevel(cond, ref = baseline)\n",
    "md <- model.matrix(~cond,s2c)\n",
    "colnames(md)[2] <- variablebaseline\n",
    "\n",
    "\n",
    "#Next we make the sleuth object. This is where all the information about the comparison is going to be in. \n",
    "so <- sleuth_prep(s2c, extra_bootstrap_summary = TRUE)\n",
    "\n",
    "#sleuth test on differences using the full and reduced model, don't really understand. you have to fill in the \"condition\" so it knows where to look for differences\n",
    "so <- sleuth_fit(so, md, 'full')\n",
    "so <- sleuth_fit(so, ~1, 'reduced')\n",
    "\n",
    "#And then the walt test is done.\n",
    "so <- sleuth_wt(so, variablebaseline, 'full')\n",
    "\n",
    "\n",
    "#If me make a “model” of this sleuth object we can read it.\n",
    " models(so)\n",
    "#make a table of the restults\n",
    "resultsWT_table <- sleuth_results(so, variablebaseline, test_type = 'wt')\n",
    "\n",
    "#sleuth gives a “b” value. This is a “bias-estimator”, it is close to a fold change when the variance is low (bootstrap and variance between samples of the same group), but if the variance is high it will lower this number, to give an indication that is not very certain of the result. Furthermore the b value is calculated with a natural logarithm instead of LOG2, which is the standard. We will add the log2 b value next\n",
    "#first take e^x to get rid of the natural logarithm\n",
    "resultsWT_table $b_e <- exp((resultsWT_table $b))\n",
    "#Then transform with log2\n",
    "resultsWT_table $b_e_log2 <- log2(resultsWT_table $b_e)\n",
    "\n",
    "\n",
    "#write the result to a file\n",
    "write.table(resultsWT_table, file.path(\"/sleuth_output \",”WtCvsWtD”, \"WaltTest.txt”), sep=\"\\t\") \n",
    "#only take the significant part of those files, (q value < 0.05) write them to a new file and show some\" \n",
    "sleuthWT_significant <- dplyr::filter(resultsWT_table, qval <= 0.05)\n",
    "write.table(resultsWT_table, file.path(\"/sleuth_output \",”WtCvsWtDSig”, \".txt”), sep=\"\\t\", row.names = FALSE) \n",
    "\n",
    "#Lastly we can open up shiny which will allow us to inspect the data in more detail.\n",
    "sleuth_live(so)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sleuth object can also be found (RNAseqlessen.Rdata)in the lessen plan if you only want to experiment with the shiny visualisation.\n",
    "under maps, PCA, you can see if the samples map together according to the experimental design\n",
    "under analysis, transcript view, you can look up some of the differntielly expressed genes that are saved in the WtCvsWtDSig.txt file. Examples to ook up are ... ... ..., which are 3 drought marker genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
